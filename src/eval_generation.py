"""
Evaluate generated answers using LLM-as-Judge.

This module implements generation quality evaluation:
1. Load generated answers with their context
2. Use GPT-4 as a judge to evaluate:
   - Faithfulness: Is the answer supported by the retrieved context?
   - Relevance: Does the answer address the question?
   - Completeness: Does it cover key points from gold answer?
   - Citation Quality: Are citations accurate and appropriate?
3. Compute aggregate metrics by question type
4. Save detailed evaluation results
"""

import json
import os
import sys
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import List, Dict, Any, Optional
import pandas as pd
from openai import OpenAI
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from rich.panel import Panel

# Try to load dotenv if available
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # Manual .env loading
    env_path = Path(__file__).parent.parent / '.env'
    if env_path.exists():
        with open(env_path) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

console = Console()


@dataclass
class JudgmentScores:
    """Scores from LLM-as-Judge evaluation."""
    faithfulness: int  # 1-5: Answer supported by context
    relevance: int     # 1-5: Answer addresses the question
    completeness: int  # 1-5: Covers key points from gold answer
    citation_quality: int  # 1-5: Citations accurate and appropriate
    explanation: str   # Judge's reasoning


@dataclass
class AnswerEvaluation:
    """Complete evaluation of a generated answer."""
    question_id: int
    question: str
    question_type: str
    gold_answer: str
    generated_answer: str
    retrieved_chunk_ids: List[int]
    cited_chunk_ids: List[int]
    scores: JudgmentScores
    metadata: Dict[str, Any]


class LLMJudge:
    """Use GPT-4 to evaluate answer quality."""
    
    def __init__(self, model_name: str = "gpt-4o-mini", temperature: float = 0.1):
        """
        Initialize the LLM judge.
        
        Args:
            model_name: OpenAI model to use for judging (gpt-4o-mini recommended)
            temperature: Low temperature for consistent judgments
        """
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment")
        
        self.client = OpenAI(api_key=api_key)
        self.model_name = model_name
        self.temperature = temperature
        
        # Load judging prompt
        self.prompt_template = self._load_prompt()
    
    def _load_prompt(self) -> str:
        """Load the LLM-as-Judge prompt template."""
        return """You are an expert evaluator assessing the quality of answers generated by a RAG (Retrieval-Augmented Generation) system.

Evaluate the generated answer on these criteria:

1. **Faithfulness** (1-5): Is the answer fully supported by the retrieved context?
   - 5: Completely supported, no hallucinations
   - 4: Mostly supported, minor unsupported details
   - 3: Partially supported, some unsupported claims
   - 2: Minimally supported, many unsupported claims
   - 1: Not supported, mostly hallucinated

2. **Relevance** (1-5): Does the answer address the question asked?
   - 5: Directly and completely answers the question
   - 4: Answers the question with minor irrelevant details
   - 3: Partially answers, some relevant information
   - 2: Barely addresses the question
   - 1: Doesn't answer the question

3. **Completeness** (1-5): Does the answer cover key points from the gold answer?
   - 5: Covers all key points, possibly adds relevant context
   - 4: Covers most key points
   - 3: Covers some key points, missing important information
   - 2: Covers few key points
   - 1: Misses most/all key points

4. **Citation Quality** (1-5): Are citations accurate and appropriate?
   - 5: All citations accurate, properly placed
   - 4: Most citations accurate
   - 3: Some citations accurate, some missing/wrong
   - 2: Few accurate citations
   - 1: No or all wrong citations

QUESTION:
{question}

GOLD ANSWER (reference):
{gold_answer}

RETRIEVED CONTEXT:
{context}

GENERATED ANSWER:
{generated_answer}

CITED CHUNKS:
{citations}

Respond with ONLY valid JSON in this format:
{{
  "faithfulness": <score 1-5>,
  "relevance": <score 1-5>,
  "completeness": <score 1-5>,
  "citation_quality": <score 1-5>,
  "explanation": "Brief explanation of scores (2-3 sentences)"
}}"""
    
    def evaluate(
        self,
        question: str,
        gold_answer: str,
        generated_answer: str,
        context_chunks: List[Dict[str, Any]],
        cited_chunk_ids: List[int]
    ) -> JudgmentScores:
        """
        Evaluate a generated answer.
        
        Args:
            question: The question being answered
            gold_answer: The reference answer
            generated_answer: The RAG-generated answer
            context_chunks: Retrieved chunks with {chunk_id, text}
            cited_chunk_ids: Chunk IDs cited in the answer
            
        Returns:
            JudgmentScores with evaluation metrics
        """
        # Format context
        context_lines = []
        for chunk in context_chunks:
            context_lines.append(f"[{chunk['chunk_id']}]: {chunk['text'][:300]}...")
        context = "\n\n".join(context_lines)
        
        # Format citations
        citations = f"Cited chunk IDs: {cited_chunk_ids}" if cited_chunk_ids else "No citations"
        
        # Build prompt
        prompt = self.prompt_template.format(
            question=question,
            gold_answer=gold_answer,
            generated_answer=generated_answer,
            context=context,
            citations=citations
        )
        
        # Call LLM judge
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an expert evaluator of RAG system answers. Respond only with valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                max_tokens=500
            )
            
            result_text = response.choices[0].message.content
            if result_text is None:
                raise ValueError("Empty response from judge")
            
            # Parse JSON
            result = json.loads(result_text.strip())
            
            return JudgmentScores(
                faithfulness=int(result.get("faithfulness", 0)),
                relevance=int(result.get("relevance", 0)),
                completeness=int(result.get("completeness", 0)),
                citation_quality=int(result.get("citation_quality", 0)),
                explanation=result.get("explanation", "")
            )
            
        except Exception as e:
            console.print(f"[yellow]Warning: Judge evaluation failed: {e}[/yellow]")
            # Return neutral scores on failure
            return JudgmentScores(
                faithfulness=0,
                relevance=0,
                completeness=0,
                citation_quality=0,
                explanation=f"Evaluation failed: {str(e)}"
            )


def evaluate_answers(
    answers_file: Path,
    output_file: Path,
    judge_model: str = "gpt-4o-mini"
) -> Dict[str, Any]:
    """
    Evaluate all generated answers using LLM-as-Judge.
    
    Args:
        answers_file: JSONL file with generated answers
        output_file: Output CSV file for evaluation results
        judge_model: Model to use for judging
        
    Returns:
        Dictionary with evaluation statistics
    """
    console.print(Panel.fit(
        f"[bold cyan]LLM-as-Judge Evaluation[/bold cyan]\n"
        f"Input: [yellow]{answers_file.name}[/yellow]\n"
        f"Judge: [yellow]{judge_model}[/yellow]",
        title="Step 8: Evaluate Generation",
        border_style="cyan"
    ))
    
    # Load generated answers
    answers = []
    with open(answers_file, 'r', encoding='utf-8') as f:
        for line in f:
            answers.append(json.loads(line))
    
    console.print(f"\nüìä Loaded {len(answers)} generated answers")
    
    # Initialize judge
    judge = LLMJudge(model_name=judge_model)
    
    # Evaluate each answer
    evaluations = []
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TimeElapsedColumn(),
        console=console
    ) as progress:
        task = progress.add_task("Evaluating answers...", total=len(answers))
        
        for i, answer_data in enumerate(answers):
            try:
                # Evaluate this answer
                scores = judge.evaluate(
                    question=answer_data['question'],
                    gold_answer=answer_data['gold_answer'],
                    generated_answer=answer_data['generated_answer'],
                    context_chunks=answer_data['context_chunks'],
                    cited_chunk_ids=answer_data['cited_chunk_ids']
                )
                
                evaluation = AnswerEvaluation(
                    question_id=answer_data.get('question_id', i),
                    question=answer_data['question'],
                    question_type=answer_data['question_type'],
                    gold_answer=answer_data['gold_answer'],
                    generated_answer=answer_data['generated_answer'],
                    retrieved_chunk_ids=answer_data['retrieved_chunk_ids'],
                    cited_chunk_ids=answer_data['cited_chunk_ids'],
                    scores=scores,
                    metadata={
                        'chunk_config': answer_data.get('chunk_config', ''),
                        'embedding_model': answer_data.get('embedding_model', ''),
                        'llm_model': answer_data.get('llm_model', '')
                    }
                )
                
                evaluations.append(evaluation)
                
            except Exception as e:
                console.print(f"[yellow]Warning: Failed to evaluate answer {i}: {e}[/yellow]")
            
            progress.update(task, advance=1)
    
    # Convert to DataFrame for analysis
    eval_data = []
    for eval_item in evaluations:
        eval_data.append({
            'question_id': eval_item.question_id,
            'question_type': eval_item.question_type,
            'question': eval_item.question,
            'gold_answer': eval_item.gold_answer,
            'generated_answer': eval_item.generated_answer,
            'faithfulness': eval_item.scores.faithfulness,
            'relevance': eval_item.scores.relevance,
            'completeness': eval_item.scores.completeness,
            'citation_quality': eval_item.scores.citation_quality,
            'avg_score': (eval_item.scores.faithfulness + eval_item.scores.relevance + 
                         eval_item.scores.completeness + eval_item.scores.citation_quality) / 4,
            'num_retrieved': len(eval_item.retrieved_chunk_ids),
            'num_cited': len(eval_item.cited_chunk_ids),
            'explanation': eval_item.scores.explanation,
            'chunk_config': eval_item.metadata.get('chunk_config', ''),
            'embedding_model': eval_item.metadata.get('embedding_model', ''),
            'llm_model': eval_item.metadata.get('llm_model', '')
        })
    
    df = pd.DataFrame(eval_data)
    
    # Save detailed results
    output_file.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_file, index=False)
    console.print(f"\n‚úì Saved detailed results to: [cyan]{output_file}[/cyan]")
    
    # Compute statistics
    stats = {
        'total_answers': len(evaluations),
        'avg_faithfulness': df['faithfulness'].mean(),
        'avg_relevance': df['relevance'].mean(),
        'avg_completeness': df['completeness'].mean(),
        'avg_citation_quality': df['citation_quality'].mean(),
        'avg_overall': df['avg_score'].mean(),
        'by_question_type': {}
    }
    
    # Stats by question type
    for qtype in df['question_type'].unique():
        type_df = df[df['question_type'] == qtype]
        stats['by_question_type'][qtype] = {
            'count': len(type_df),
            'avg_faithfulness': type_df['faithfulness'].mean(),
            'avg_relevance': type_df['relevance'].mean(),
            'avg_completeness': type_df['completeness'].mean(),
            'avg_citation_quality': type_df['citation_quality'].mean(),
            'avg_overall': type_df['avg_score'].mean()
        }
    
    # Display summary
    _display_summary(stats, df)
    
    return stats


def _display_summary(stats: Dict[str, Any], df: pd.DataFrame):
    """Display evaluation summary."""
    console.print("\n[bold green]üìä Evaluation Summary:[/bold green]\n")
    
    # Overall metrics
    table = Table(title="Overall Scores (1-5 scale)", show_header=True, header_style="bold cyan")
    table.add_column("Metric", style="cyan")
    table.add_column("Score", justify="right", style="green")
    table.add_column("Grade", justify="center")
    
    def get_grade(score):
        if score >= 4.5: return "üåü Excellent"
        if score >= 4.0: return "‚úÖ Very Good"
        if score >= 3.5: return "üëç Good"
        if score >= 3.0: return "‚ö†Ô∏è  Fair"
        return "‚ùå Poor"
    
    table.add_row("Faithfulness", f"{stats['avg_faithfulness']:.2f}", get_grade(stats['avg_faithfulness']))
    table.add_row("Relevance", f"{stats['avg_relevance']:.2f}", get_grade(stats['avg_relevance']))
    table.add_row("Completeness", f"{stats['avg_completeness']:.2f}", get_grade(stats['avg_completeness']))
    table.add_row("Citation Quality", f"{stats['avg_citation_quality']:.2f}", get_grade(stats['avg_citation_quality']))
    table.add_row("[bold]Overall Average[/bold]", f"[bold]{stats['avg_overall']:.2f}[/bold]", 
                  f"[bold]{get_grade(stats['avg_overall'])}[/bold]")
    
    console.print(table)
    
    # By question type
    if stats['by_question_type']:
        console.print("\n[bold cyan]üìã Scores by Question Type:[/bold cyan]")
        
        type_table = Table(show_header=True, header_style="bold cyan")
        type_table.add_column("Type", style="cyan")
        type_table.add_column("Count", justify="right")
        type_table.add_column("Faithful", justify="right")
        type_table.add_column("Relevant", justify="right")
        type_table.add_column("Complete", justify="right")
        type_table.add_column("Citations", justify="right")
        type_table.add_column("Overall", justify="right", style="bold")
        
        for qtype, qstats in stats['by_question_type'].items():
            type_table.add_row(
                qtype,
                str(qstats['count']),
                f"{qstats['avg_faithfulness']:.2f}",
                f"{qstats['avg_relevance']:.2f}",
                f"{qstats['avg_completeness']:.2f}",
                f"{qstats['avg_citation_quality']:.2f}",
                f"{qstats['avg_overall']:.2f}"
            )
        
        console.print(type_table)
    
    # Top and bottom performers
    console.print("\n[bold cyan]üèÜ Top 3 Answers:[/bold cyan]")
    top3 = df.nlargest(3, 'avg_score')[['question_type', 'question', 'avg_score']]
    for idx, row in top3.iterrows():
        console.print(f"  {row['avg_score']:.2f} - [{row['question_type']}] {row['question'][:60]}...")
    
    console.print("\n[bold yellow]‚ö†Ô∏è  Bottom 3 Answers:[/bold yellow]")
    bottom3 = df.nsmallest(3, 'avg_score')[['question_type', 'question', 'avg_score']]
    for idx, row in bottom3.iterrows():
        console.print(f"  {row['avg_score']:.2f} - [{row['question_type']}] {row['question'][:60]}...")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Evaluate generated answers using LLM-as-Judge")
    parser.add_argument("--answers-file", type=Path, required=True,
                       help="JSONL file with generated answers")
    parser.add_argument("--output-file", type=Path, 
                       help="Output CSV file (default: runs/generation/evaluation_<input>.csv)")
    parser.add_argument("--judge-model", type=str, default="gpt-4o-mini",
                       help="Model to use as judge (default: gpt-4o-mini)")
    
    args = parser.parse_args()
    
    # Default output file
    if args.output_file is None:
        answers_stem = args.answers_file.stem.replace('answers__', 'evaluation__')
        args.output_file = Path("runs/generation") / f"{answers_stem}.csv"
    
    try:
        stats = evaluate_answers(
            answers_file=args.answers_file,
            output_file=args.output_file,
            judge_model=args.judge_model
        )
        
        console.print(f"\n[bold green]‚úì Evaluation complete![/bold green]")
        console.print(f"Overall quality score: [bold cyan]{stats['avg_overall']:.2f}/5.0[/bold cyan]")
        
    except Exception as e:
        console.print(f"\n[bold red]Error:[/bold red] {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
